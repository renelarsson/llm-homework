{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b659bce6",
   "metadata": {},
   "source": [
    "### Homework: Search Evaluation\n",
    "---\n",
    "In this homework, we will evaluate the results of vector search.\n",
    "> It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "\n",
    "#### Required libraries\n",
    "---\n",
    "We will use minsearch and Qdrant. Make sure you have the most up-to-date versions:\n",
    "\n",
    "`pip install -U minsearch qdrant_client`\n",
    "\n",
    "minsearch should be at least 0.0.4.\n",
    "\n",
    "#### Evaluation data\n",
    "---\n",
    "For this homework, we will use the same dataset we generated in the videos.\n",
    "Let's get them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1b5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "#!pip install -U minsearch qdrant_client rouge scikit-learn tqdm pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e63f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load evaluation data\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "# Creates the full URL for the JSON file containing the documents to be used for evaluation\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "# Downloads the JSON file from the URL and loads it into a Python list of dictionaries\n",
    "documents = requests.get(docs_url).json()\n",
    "# Creates the full URL for the CSV file containing ground truth data for evaluation\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "# Downloads the CSV file and loads it into a pandas DataFrame\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "# Converts the DataFrame to a list of dictionaries for easier access\n",
    "# Each dictionary represents a row/record in the DataFrame, with column names as keys\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec90e3a",
   "metadata": {},
   "source": [
    "Here, `documents` contains the documents from the FAQ database with unique IDs, and `ground_truth` contains generated question-answer pairs.\n",
    "\n",
    "Also, we will need the code for evaluating retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216bf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics: hit_rate, mrr, and evaluate function\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Calculates the fraction of queries for which at least one relevant document was retrieved\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "    # For each query/line, check if any result is True (i.e., a relevant document was found)\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "# Calculates the Mean Reciprocal Rank of the relevant documents across all queries (quality ranking metric)\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    # For each query, find the rank/position of the first relevant document (where line[rank] is True)\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "# Evaluates the search function against the ground truth data\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    # For each query in ground_truth, get the document ID and retrieve search results\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        # Call the search function with the query text\n",
    "        results = search_function(q)\n",
    "        # Check if the result’s ID matches the ground truth document ID\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        # Append the relevance list to the total relevance list\n",
    "        relevance_total.append(relevance)\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5ea7d",
   "metadata": {},
   "source": [
    "#### Q1. Minsearch text\n",
    "---\n",
    "Now let's evaluate our usual minsearch approach (*i.e. perform text search with boosting parameters for 'question' and 'section' fields*), but tweak the parameters. Let's use the following boosting params:\n",
    "```python\n",
    "boost = {'question': 1.5, 'section': 0.1}\n",
    "```\n",
    "\n",
    "What's the hitrate for this approach?\n",
    "\n",
    "* 0.64\n",
    "* 0.74\n",
    "* 0.84\n",
    "* 0.94\n",
    "\n",
    "##### Embeddings\n",
    "---\n",
    "The latest version of minsearch also supports vector search. We will use it:\n",
    "\n",
    "```python\n",
    "from minsearch import VectorSearch\n",
    "```\n",
    "\n",
    "We will also use TF-IDF and Singular Value Decomposition to create embeddings from texts. You can refer to our [\"Create Your Own Search Engine\" workshop](https://github.com/alexeygrigorev/build-your-own-search-engine) if you want to know more about it.\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "```\n",
    "\n",
    "Let's create embeddings for the \"question\" field:\n",
    "```python\n",
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68019247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Minsearch text search with boosting parameters\n",
    "from minsearch import Index\n",
    "\n",
    "# Creates a search index using the minsearch library\n",
    "index = Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"], # Full-text search in each document\n",
    "    keyword_fields=[\"course\", \"id\"] # Filtering by course or id\n",
    ")\n",
    "index.fit(documents) # Build the index from the list of documents\n",
    "\n",
    "# Searches the index for documents matching the query and course\n",
    "def minsearch_search(query, course):\n",
    "    boost = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course}, # Filters results to only those matching the given course\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# Evaluate hitrate for minsearch text search\n",
    "relevance_total = []\n",
    "# For each query in the ground truth data, get the document id and search using the question and course\n",
    "for q in ground_truth:\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_search(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results] # Check if results match the expected document id\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa7d759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.848714069591528, 0.7288235717887772)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total), mrr(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98840f5",
   "metadata": {},
   "source": [
    "## Q2. Vector search for question\n",
    "\n",
    "Now let's index these embeddings with minsearch:\n",
    "```python\n",
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)\n",
    "```\n",
    "Evaluate this seach method. What's MRR for it?\n",
    "\n",
    "* 0.25\n",
    "* 0.35\n",
    "* 0.45\n",
    "* 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6558f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce1e2296241427a9674b1da6ace8e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector search MRR: 0.3572833369353793\n"
     ]
    }
   ],
   "source": [
    "# Q2. Vector search for question field using TF-IDF and SVD embeddings\n",
    "from minsearch import VectorSearch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create embeddings for 'question' field \n",
    "texts = []\n",
    "# Extracts the value of the 'question' field from each document and appends it to the texts list\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "# Create an Embedding Pipeline\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3), # TF-IDF embedding ignores words that appear in fewer than 3 documents\n",
    "    TruncatedSVD(n_components=128, random_state=1) # SVD embedding reduces TF-IDF vector dimensionality to 128\n",
    " )\n",
    "X = pipeline.fit_transform(texts) # Fit the pipeline to the texts and transform them into dense vectors\n",
    "\n",
    "# Index embeddings with minsearch VectorSearch\n",
    "vindex = VectorSearch(keyword_fields={'course'}) # Initialize a VectorSearch index filtering through 'course'\n",
    "vindex.fit(X, documents) # Build the index using the embeddings (X) and the original documents\n",
    "\n",
    "def vector_search(q): # Query q is a dictionary with at least 'question' and 'course' fields\n",
    "    # Transform the question to embedding\n",
    "    v_q = pipeline.transform([q['question']]) # Transforms the query’s question into an embedding\n",
    "    results = vindex.search(v_q, {'course': q['course']}, 5) # Search the top 5 indicies filtering by course\n",
    "    return results\n",
    "\n",
    "# Evaluate MRR for vector search on question field\n",
    "vector_search_mrr = evaluate(ground_truth, vector_search)\n",
    "print('Vector search MRR:', vector_search_mrr['mrr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4d1bf",
   "metadata": {},
   "source": [
    "#### Q3. Vector search for question and answer \n",
    "\n",
    "We only used question in Q2. We can use both question and answer:\n",
    "```python\n",
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question'] + ' ' + doc['text']\n",
    "    texts.append(t)\n",
    "```\n",
    "Using the same pipeline (`min_df=3` for TF-IDF vectorizer and `n_components=128` for SVD), evaluate the performance of this approach\n",
    "\n",
    "What's the hitrate?\n",
    "\n",
    "* 0.62\n",
    "* 0.72\n",
    "* 0.82\n",
    "* 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2550d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d033f86a734082a106f21bf1ad2ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 - Vector search (question+answer) hitrate: 0.8210503566025502\n"
     ]
    }
   ],
   "source": [
    "# Q3. Vector search for question and answer fields\n",
    "texts_qa = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "pipeline_qa = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    " )\n",
    "X_qa = pipeline_qa.fit_transform(texts_qa)\n",
    "\n",
    "vindex_qa = VectorSearch(keyword_fields={'course'})\n",
    "vindex_qa.fit(X_qa, documents)\n",
    "\n",
    "def vector_search_question_answer(q):\n",
    "    # Only use the question field from the query, since ground_truth does not have 'text'\n",
    "    v_q = pipeline_qa.transform([q['question']])\n",
    "    results = vindex_qa.search(v_q, {'course': q['course']}, 5)\n",
    "    return results\n",
    "\n",
    "# Evaluate hitrate for vector search on question+answer\n",
    "result_q3 = evaluate(ground_truth, vector_search_question_answer)\n",
    "print('Q3 - Vector search (question+answer) hitrate:', result_q3['hit_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d40d45",
   "metadata": {},
   "source": [
    "#### Q4. Qdrant \n",
    "\n",
    "Now let's evaluate the following settings in Qdrant:\n",
    "\n",
    "* `text = doc['question'] + ' ' + doc['text']`\n",
    "* `model_handle = \"jinaai/jina-embeddings-v2-small-en\"`\n",
    "* `limit = 5`\n",
    "\n",
    "What's the MRR?\n",
    "\n",
    "* 0.65\n",
    "* 0.75\n",
    "* 0.85\n",
    "* 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "205c77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b60f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_26321/4139094074.py:14: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d989aa013340403189a43f42a36b2a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26321/4139094074.py:34: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 - Qdrant vector search MRR: 0.08784309487789052\n"
     ]
    }
   ],
   "source": [
    "# Q4. Qdrant vector search using Jina embeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "\n",
    "# Use Jina embeddings (requires jinaai/jina-embeddings-v2-small-en)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "embedding_model = SentenceTransformer(model_handle)\n",
    "\n",
    "# Prepare Qdrant collection\n",
    "client = QdrantClient(':memory:')\n",
    "collection_name = \"search_eval\"\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=embedding_model.get_sentence_embedding_dimension(),\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# Index documents in Qdrant using integer IDs\n",
    "points = []\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    vector = embedding_model.encode(text)\n",
    "    points.append(PointStruct(id=i, vector=vector, payload=doc))\n",
    "client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "def qdrant_search(q):\n",
    "    # Only use the question field from the query, since ground_truth does not have 'text'\n",
    "    text = q['question'] \n",
    "    vector = embedding_model.encode(text)\n",
    "    hits = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=vector,\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    return [hit.payload for hit in hits]\n",
    "\n",
    "# Evaluate MRR for Qdrant search\n",
    "result_q4 = evaluate(ground_truth, qdrant_search)\n",
    "print('Q4 - Qdrant vector search MRR:', result_q4['mrr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee75b3e",
   "metadata": {},
   "source": [
    "#### Q5. Cosine similarity\n",
    "\n",
    "In the second part of the module, we looked at evaluating the entire RAG approach. In particular, we looked at comparing the answer generated by our system with the actual answer from the FAQ.\n",
    "\n",
    "One of the ways of doing it is using the cosine similarity. Let's see how to calculate it.\n",
    "\n",
    "Cosine similarity is a dot product between two normalized vectors. In geometrical sense, it's the cosine of the angle between the vectors. Look up \"cosine similarity geometry\" if you want to learn more about it.\n",
    "\n",
    "For us, it means that we need two things:\n",
    "\n",
    "* First, we normalize each of the vectors\n",
    "* Then, compute the dot product\n",
    "\n",
    "So, we get this:\n",
    "```python\n",
    "def cosine(u, v):\n",
    "    u = normalize(u)\n",
    "    v = normalize(v)\n",
    "    return u.dot(v)\n",
    "```\n",
    "For normalization, we first compute the vector norm (its length), and then divide the vector by it:\n",
    "```python\n",
    "def normalize(u):\n",
    "    norm = np.sqrt(u.dot(u))\n",
    "    return u / norm\n",
    "```\n",
    "(where `np` is `import numpy as np`)\n",
    "\n",
    "Or we can simplify it:\n",
    "```python\n",
    "def cosine(u, v):\n",
    "    u_norm = np.sqrt(u.dot(u))\n",
    "    v_norm = np.sqrt(v.dot(v))\n",
    "    return u.dot(v) / (u_norm * v_norm)\n",
    "```\n",
    "Now let's use this function to compute the A->Q->A cosine similarity.\n",
    "\n",
    "We will use the results from [our gpt-4o-mini evaluations](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/03-evaluation/rag_evaluation/data/results-gpt4o-mini.csv):\n",
    "```python\n",
    "results_url = url_prefix + 'rag_evaluation/data/results-gpt4o-mini.csv'\n",
    "df_results = pd.read_csv(results_url)\n",
    "```\n",
    "\n",
    "When creating embeddings, we will use a simple way - the same we used in the [Embeddings](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2025/03-evaluation/homework.md#embeddings) section:\n",
    "```python\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "```\n",
    "Let's fit the vectorizer on all the text data we have:\n",
    "```python\n",
    "pipeline.fit(df_results.answer_llm + ' ' + df_results.answer_orig + ' ' + df_results.question)\n",
    "```\n",
    "\n",
    "Now use the `transform` methon of the pipeline to create the embeddings and calculate the cosine similarity between each pair.\n",
    "\n",
    "What's the average cosine?\n",
    "\n",
    "* 0.64\n",
    "* 0.74\n",
    "* 0.84\n",
    "* 0.94\n",
    "\n",
    "This is how you do it:\n",
    "\n",
    "* For each answer pair, compute\n",
    "    * `v_llm` for the answer from the LLM\n",
    "    * `v_orig` for the original answer\n",
    "    * then compute the cosine between them\n",
    "* At the end, take the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6d506b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 - Average cosine similarity: 0.7463632445867671\n"
     ]
    }
   ],
   "source": [
    "# Q5. Cosine similarity calculation for LLM and original answers\n",
    "results_url = url_prefix + 'rag_evaluation/data/results-gpt4o-mini.csv'\n",
    "df_results = pd.read_csv(results_url)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Fit pipeline on all text data\n",
    "all_texts = pd.concat([df_results['answer_llm'], df_results['answer_orig'], df_results['question']])\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "pipeline.fit(all_texts)\n",
    "\n",
    "def normalize(u):\n",
    "    norm = np.sqrt(u.dot(u))\n",
    "    return u / norm\n",
    "\n",
    "def cosine(u, v):\n",
    "    u = normalize(u)\n",
    "    v = normalize(v)\n",
    "    return u.dot(v)\n",
    "\n",
    "cosines = []\n",
    "for _, row in df_results.iterrows():\n",
    "    v_llm = pipeline.transform([row['answer_llm']])[0]\n",
    "    v_orig = pipeline.transform([row['answer_orig']])[0]\n",
    "    cos_val = cosine(v_llm, v_orig)\n",
    "    cosines.append(cos_val)\n",
    "\n",
    "avg_cosine = np.mean(cosines)\n",
    "print('Q5 - Average cosine similarity:', avg_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc257ba",
   "metadata": {},
   "source": [
    "#### Q6. Rouge\n",
    "\n",
    "And alternative way to see how two texts are similar is ROUGE.\n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n",
    "`pip install rouge`\n",
    "\n",
    "(The latest version at the moment of writing is `1.0.1`)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)\n",
    "```python\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "r = df_results.iloc[10]\n",
    "scores = rouge_scorer.get_scores(r.answer_llm, r.answer_orig)[0]\n",
    "scores\n",
    "```\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "* `rouge-1` - the overlap of unigrams,\n",
    "* `rouge-2` - bigrams,\n",
    "* `rouge-l` - the longest common subsequence\n",
    "\n",
    "For the 10th document, Rouge-1 F1 score is 0.45\n",
    "\n",
    "Let's compute it for the pairs in the entire dataframe. What's the average Rouge-1 F1?\n",
    "\n",
    "* 0.25\n",
    "* 0.35\n",
    "* 0.45\n",
    "* 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfe96ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6 - ROUGE-1 F1 for 10th document: 0.45454544954545456\n",
      "Q6 - Average ROUGE-1 F1: 0.3516946452113943\n"
     ]
    }
   ],
   "source": [
    "# Q6. ROUGE score calculation for LLM and original answers\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "# ROUGE-1 F1 for the 10th document (index 10)\n",
    "r = df_results.iloc[10]\n",
    "scores_10 = rouge_scorer.get_scores(r.answer_llm, r.answer_orig)[0]\n",
    "rouge_1_f1_10 = scores_10['rouge-1']['f']\n",
    "print('Q6 - ROUGE-1 F1 for 10th document:', rouge_1_f1_10)\n",
    "\n",
    "# Average ROUGE-1 F1 for all pairs\n",
    "rouge_1_f1_scores = []\n",
    "for _, row in df_results.iterrows():\n",
    "    scores = rouge_scorer.get_scores(row['answer_llm'], row['answer_orig'])[0]\n",
    "    rouge_1_f1_scores.append(scores['rouge-1']['f'])\n",
    "\n",
    "avg_rouge_1_f1 = np.mean(rouge_1_f1_scores)\n",
    "print('Q6 - Average ROUGE-1 F1:', avg_rouge_1_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
